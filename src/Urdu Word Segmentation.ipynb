{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Urdu Word Segmentation\n",
    "Let's start by loading in the packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import unicodedata\n",
    "import pycrfsuite\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of sentences in the dataset: 4325\n"
     ]
    }
   ],
   "source": [
    "sentences = list()\n",
    "f = codecs.open('Data/data_diacritized.txt', 'r', encoding='utf-8')\n",
    "for line in f:\n",
    "    line = unicodedata.normalize('NFC',line)\n",
    "    line = re.sub(r\"\\s{2,}\", \" \", line)\n",
    "    #Comment below 7 lines to get results with diacritization\n",
    "    line = re.sub(u\"ِ\", \"\", line)\n",
    "    line = re.sub(u\"ُ\", \"\", line)\n",
    "    line = re.sub(u\"َ\", \"\", line)\n",
    "    line = re.sub(u\"ْ\", \"\", line)\n",
    "    line = re.sub(u\"ٰ\", \"\", line)\n",
    "    line = re.sub(u\"ً\", \"\", line)\n",
    "    line = re.sub(u\"ّ\", \"\", line)\n",
    "    sentences.append(line)\n",
    "f.close()\n",
    "\n",
    "print (\"No. of sentences in the dataset:\",len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tokens in test set: 21088\n",
      "No. of tokens in train set: 90151\n"
     ]
    }
   ],
   "source": [
    "test_tokens = list()\n",
    "train_tokens = list()\n",
    "for i in sentences[-825:]:\n",
    "    test_tokens.extend(i.split())\n",
    "for i in sentences[:-825]:\n",
    "    train_tokens.extend(i.split())\n",
    "print (\"No. of tokens in test set:\",len(test_tokens))\n",
    "print (\"No. of tokens in train set:\",len(train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare sentences for training by removing 'white spaces' and 'zero width non-joiner'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence):\n",
    "    \n",
    "    lengths = [len(w) for w in sentence.split(\" \")]\n",
    "    positions = []\n",
    "\n",
    "    next_pos = 0\n",
    "    for length in lengths:\n",
    "        next_pos = next_pos + length\n",
    "        positions.append(next_pos)\n",
    "    concatenated = sentence.replace(\" \", \"\")\n",
    "\n",
    "    chars = [c for c in concatenated]\n",
    "    labels = [0 if not i in positions else 1 for i, c in enumerate(concatenated)]\n",
    "    \n",
    "    for i, c in enumerate(chars):\n",
    "        if c == u\"\\u200C\" and i+1 < len(chars):\n",
    "            labels[i+1] = 2\n",
    "            del chars[i]\n",
    "            del labels[i]\n",
    "    \n",
    "    return list(zip(chars, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_sentences = list()\n",
    "for sentence in sentences:    \n",
    "    prepared_sentences.append(prepare_sentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Next, define some features.\n",
    "- N-grams consisting of the current character and up to three preceding and three succeeding characters\n",
    "- Whether the current character is a digit\n",
    "- Whether the current character is a joiner\n",
    "- Unicode class of current character\n",
    "- Direction of current character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkdigit(char):\n",
    "    digits = [u'۱',u'۲',u'۳',u'۴',u'۵',u'۶',u'۷',u'۸',u'۹',u'۰']\n",
    "    if char in digits:\n",
    "        return \"true\"\n",
    "    return \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnonjoiner(char):\n",
    "    non_joiners = [u'ا', u'د', u'ڈ', u'ز', u'ذ', u'ر', u'ڑ', u'ژ', u'و', u'ے']\n",
    "    if char in non_joiners:\n",
    "        return \"true\"\n",
    "    return \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_features(sentence, i):\n",
    "    features = [\n",
    "        'bias',\n",
    "        'char=' + sentence[i][0],\n",
    "        'char.isdigit=' + checkdigit(sentence[i][0]),\n",
    "        'char.isnonjoiner=' + isnonjoiner(sentence[i][0]),\n",
    "        'char.category=' + unicodedata.category(sentence[i][0]),\n",
    "        'char.direction=' + unicodedata.bidirectional(sentence[i][0]),\n",
    "    ]\n",
    "    \n",
    "    if i >= 1:\n",
    "        features.extend([\n",
    "            'char-1=' + sentence[i-1][0],\n",
    "            'char-1:0=' + sentence[i-1][0] + sentence[i][0],\n",
    "        ])\n",
    "    else:\n",
    "        features.append(\"BOS\")\n",
    "        \n",
    "    if i >= 2:\n",
    "        features.extend([\n",
    "            'char-2=' + sentence[i-2][0],\n",
    "            'char-2:0=' + sentence[i-2][0] + sentence[i-1][0] + sentence[i][0],\n",
    "            'char-2:-1=' + sentence[i-2][0] + sentence[i-1][0],\n",
    "        ])\n",
    "        \n",
    "    if i >= 3:\n",
    "        features.extend([\n",
    "            'char-3:0=' + sentence[i-3][0] + sentence[i-2][0] + sentence[i-1][0] + sentence[i][0],\n",
    "            'char-3:-1=' + sentence[i-3][0] + sentence[i-2][0] + sentence[i-1][0],\n",
    "        ])\n",
    "        \n",
    "        \n",
    "    if i + 1 < len(sentence):\n",
    "        features.extend([\n",
    "            'char+1=' + sentence[i+1][0],\n",
    "            'char:+1=' + sentence[i][0] + sentence[i+1][0],\n",
    "        ])\n",
    "    else:\n",
    "        features.append(\"EOS\")\n",
    "        \n",
    "    if i + 2 < len(sentence):\n",
    "        features.extend([\n",
    "            'char+2=' + sentence[i+2][0],\n",
    "            'char:+2=' + sentence[i][0] + sentence[i+1][0] + sentence[i+2][0],\n",
    "            'char+1:+2=' + sentence[i+1][0] + sentence[i+2][0],\n",
    "        ])\n",
    "        \n",
    "    if i + 3 < len(sentence):\n",
    "        features.extend([\n",
    "            'char:+3=' + sentence[i][0] + sentence[i+1][0] + sentence[i+2][0]+ sentence[i+3][0],\n",
    "            'char+1:+3=' + sentence[i+1][0] + sentence[i+2][0] + sentence[i+3][0],\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_sentence_features(prepared_sentence):\n",
    "    return [create_char_features(prepared_sentence, i) for i in range(len(prepared_sentence))]\n",
    "\n",
    "def create_sentence_labels(prepared_sentence):\n",
    "    return [str(part[1]) for part in prepared_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [create_sentence_features(ps) for ps in prepared_sentences[:-825]]\n",
    "y_train = [create_sentence_labels(ps)   for ps in prepared_sentences[:-825]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "To train the model, we create pycrfsuite.Trainer, load the training data and call 'train' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set training parameters. We will use L-BFGS training algorithm with Elastic Net (L1 + L2) regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,  # coefficient for L1 penalty\n",
    "    'c2': 1e-3, # coefficient for L2 penalty\n",
    "    'max_iterations': 60, # stop earlier\n",
    "    'feature.possible_transitions': True # include transitions that are possible, but not observed\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train('Model/urdu-word-segmentation.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "To use the trained model, create pycrfsuite.Tagger, open the model and use 'tag' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x11ca0ca00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('Model/urdu-word-segmentation.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's segment a sentence to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sentence(sentence):\n",
    "    sentence = sentence.replace(\" \", \"\")\n",
    "    sentence = sentence.replace(u\"\\u200C\", \"\") \n",
    "    prediction = tagger.tag(create_sentence_features(sentence))\n",
    "    print (prediction)\n",
    "    complete = \"\"\n",
    "    for i, p in enumerate(prediction):\n",
    "        if p == \"1\":\n",
    "            complete += \" \" + sentence[i]\n",
    "        elif p == \"2\":\n",
    "            complete += u\"\\u200C\" + sentence[i]\n",
    "        else:\n",
    "            complete += sentence[i]\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '2', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0']\n",
      "ہم ایک آزاد براہ ِراست سرمایہ‌کاری پالیسی کو جاری رکھنے کی لمبی تاریخ رکھتے ہیں مِسٹر دلارا کہتے ہیں\n"
     ]
    }
   ],
   "source": [
    "#Segment Sentence with Diacritics\n",
    "print(segment_sentence(u\"ہم ایک آزاد براہِ‌راست سرمایہ‌کاری پالیسی کو جاری رکھنے کی لمبی تاریخ رکھتے ہیں مِسٹر دلارا کہتے ہیں\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '2', '0', '0', '0', '1', '0', '0', '0', '0', '0', '2', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0']\n",
      "ہم ایک آزاد براہ‌راست سرمایہ‌کاری پالیسی کو جاری رکھنے کی لمبی تاریخ رکھتے ہیں مسٹر دلارا کہتے ہیں\n"
     ]
    }
   ],
   "source": [
    "#Segment Sentence without Diacritics\n",
    "print(segment_sentence(u\"ہم ایک آزاد براہ‌راست سرمایہ‌کاری پالیسی کو جاری رکھنے کی لمبی تاریخ رکھتے ہیں مسٹر دلارا کہتے ہیں\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "Segment all sentences in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list()\n",
    "y_pred = list()\n",
    "for s in prepared_sentences[-825:]:\n",
    "    prediction = tagger.tag(create_sentence_features(s))\n",
    "    y_pred.extend(prediction)\n",
    "    correct = create_sentence_labels(s)\n",
    "    y_true.extend(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print classification report to check results. Here \n",
    "'I' denote continuation of a word or sub-word,\n",
    "'Bw' denote beginning of a word,\n",
    "'Bs' denote beginning of a sub-word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           I       0.99      0.99      0.99     59665\n",
      "          Bw       0.97      0.97      0.97     20264\n",
      "          Bs       0.91      0.80      0.85      1200\n",
      "\n",
      "    accuracy                           0.98     81129\n",
      "   macro avg       0.96      0.92      0.94     81129\n",
      "weighted avg       0.98      0.98      0.98     81129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['I', 'Bw', 'Bs']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print confusion matrix to see class wise stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59149   474    42]\n",
      " [  574 19637    53]\n",
      " [  119   116   965]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true, y_pred, labels=[\"0\", \"1\", \"2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
